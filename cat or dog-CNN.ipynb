{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d09d050-868c-43e2-81dd-24f9c398d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc05b71f-9fe3-4925-b437-23f782ed71a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0364f849-2f2a-4845-abdf-0ad5319a5b0a",
   "metadata": {},
   "source": [
    "image augemantation perform krenge \n",
    "mtlb pixels ko istrha shigt krenge ke model overfit na hou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab62ed03-8189-45b9-b837-c873126a1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "        )\n",
    "\n",
    "# shear,zoom,horizontal ye sari transformation hain\n",
    "# rescale is feature scaling ye sb pixel ko bw 0 & 1  m krdega divide krkr cuz each pixel takes 0 to 255 so by dividing itll convert bw0md1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "465ca27c-3f27-474a-903a-a558919d6879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set=train_datagen.flow_from_directory(\n",
    "    'training_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    "\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752eb17d-e6ca-4544-8c05-be6ec2090432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image resizing to reduce computation power even with lower size well have amazing result\n",
    "\n",
    "# target size is final size of ur image 150,150 will make training very long\n",
    "\n",
    "# batchzise is eik batch m kitni pics hni chhye and its 32\n",
    "\n",
    "\n",
    "# class mode is binary h ya categorical in or case binary bcz cat and dog\n",
    "\n",
    "# upar train ki preprocessing thi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14e80f32-8c91-4768-9105-305b2b086b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pe bs rescale krenge \n",
    "# sheer horizontal etc kuch ni krenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fce21a13-a52b-4a3d-be7a-90c57da92694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_set=test_datagen.flow_from_directory(\n",
    "    'test_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb47960d-5629-44da-9b4b-8007894a8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b95a5dee-9bba-410d-8532-b73237dd7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8ec7371-ad90-4724-89e9-dbf7519c7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phele filyer ka argument ayefa then size ka we're using colors images a 3d array and we set size 64,64 tu yhn bhi yehi size use krenge\n",
    "# step1 convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "550a99b9-276a-4e05-a393-6ad54baf2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,\n",
    "                              activation='relu',\n",
    "                              input_shape=[64,64,3]))\n",
    "\n",
    "# input shape m 3 bcz coored images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d2bb9a6-9feb-4891-b38e-eccf61ff276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.add(tf.keras.layers.Input(shape=[64, 64, 3]))  # Input layer for 64x64 RGB images (3 channels)\n",
    "\n",
    "# # Add the Conv2D layer\n",
    "# cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b71fe3-2e31-4e86-954e-bdde5a516115",
   "metadata": {},
   "source": [
    "step2 pooling\n",
    "max pool banare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b0b7a96-6482-494a-9c9f-7840c9d5c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6af15260-83d0-4a7d-a7ee-be38fae14ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wohi example m 4 values ko dekho aur move kro 2 ke dsitance se\n",
    "# This defines the size of the pooling window. In this case, the pooling window is 2x2, meaning the layer looks at non-overlapping 2x2 regions of the input at a time.\n",
    "\n",
    "# stride mtlb move kese hoga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40531ed3-22c6-4c32-ae90-4f6dbbefff70",
   "metadata": {},
   "source": [
    "ading another convolutional and the again pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ba361f1-891b-4c96-a64d-b2da667a4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,\n",
    "                              activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1aa32-994d-4e90-b39e-0a89e814daf6",
   "metadata": {},
   "source": [
    "step 3 flatenning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd535aa0-85f2-4d96-bd1b-b18dbc5e48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616ea9b-e9a1-4fa8-8317-452e7d34a30e",
   "metadata": {},
   "source": [
    "step4 full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2927dd2e-4922-4f8b-ad6b-482798308ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zida nureal nets use krenge cause we're traning a cnn odel which is complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abf4c025-0494-4061-8df4-b0000c71f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259ec0b-ac15-4051-8580-5dd70d91020e",
   "metadata": {},
   "source": [
    "step 5 output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99859b27-a6fc-483b-af3b-a16358980359",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# binary krre hain jbhi sigmoid leliya wrna agar hum multi class ki krte tou hum cross entropy lete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556a605-6cbe-41fe-9a5d-3e49f493953f",
   "metadata": {},
   "source": [
    "step 6 training the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dc74cc3-9b9e-48ca-bb7c-a60683445a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling\n",
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b2b85ff-cbb9-437e-9751-3646712eb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting on train set and then evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55d128c0-fdef-4e86-a9c3-0250fd4731bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FatimaTasneem\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710ms/step - accuracy: 0.5085 - loss: 0.7122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FatimaTasneem\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 848ms/step - accuracy: 0.5086 - loss: 0.7121 - val_accuracy: 0.5765 - val_loss: 0.6681\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 319ms/step - accuracy: 0.6421 - loss: 0.6434 - val_accuracy: 0.6850 - val_loss: 0.5915\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 336ms/step - accuracy: 0.6984 - loss: 0.5881 - val_accuracy: 0.6935 - val_loss: 0.5807\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 318ms/step - accuracy: 0.7139 - loss: 0.5599 - val_accuracy: 0.7355 - val_loss: 0.5230\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 323ms/step - accuracy: 0.7412 - loss: 0.5283 - val_accuracy: 0.7565 - val_loss: 0.5144\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 304ms/step - accuracy: 0.7506 - loss: 0.5029 - val_accuracy: 0.7655 - val_loss: 0.4955\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 319ms/step - accuracy: 0.7731 - loss: 0.4710 - val_accuracy: 0.7785 - val_loss: 0.4750\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 317ms/step - accuracy: 0.7895 - loss: 0.4471 - val_accuracy: 0.7875 - val_loss: 0.4534\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 304ms/step - accuracy: 0.7878 - loss: 0.4384 - val_accuracy: 0.7990 - val_loss: 0.4435\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 303ms/step - accuracy: 0.8061 - loss: 0.4206 - val_accuracy: 0.7895 - val_loss: 0.4624\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 299ms/step - accuracy: 0.8225 - loss: 0.3945 - val_accuracy: 0.8070 - val_loss: 0.4291\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 286ms/step - accuracy: 0.8289 - loss: 0.3855 - val_accuracy: 0.8020 - val_loss: 0.4502\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 280ms/step - accuracy: 0.8320 - loss: 0.3690 - val_accuracy: 0.8055 - val_loss: 0.4370\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 369ms/step - accuracy: 0.8309 - loss: 0.3795 - val_accuracy: 0.8010 - val_loss: 0.4366\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 320ms/step - accuracy: 0.8357 - loss: 0.3584 - val_accuracy: 0.8115 - val_loss: 0.4237\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 320ms/step - accuracy: 0.8433 - loss: 0.3490 - val_accuracy: 0.8175 - val_loss: 0.4165\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 334ms/step - accuracy: 0.8521 - loss: 0.3266 - val_accuracy: 0.8155 - val_loss: 0.4188\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 360ms/step - accuracy: 0.8544 - loss: 0.3212 - val_accuracy: 0.8310 - val_loss: 0.3953\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 317ms/step - accuracy: 0.8607 - loss: 0.3121 - val_accuracy: 0.7980 - val_loss: 0.4670\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 300ms/step - accuracy: 0.8668 - loss: 0.3055 - val_accuracy: 0.8145 - val_loss: 0.4337\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 282ms/step - accuracy: 0.8752 - loss: 0.2848 - val_accuracy: 0.8280 - val_loss: 0.4183\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - accuracy: 0.8767 - loss: 0.2812 - val_accuracy: 0.8150 - val_loss: 0.4426\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 186ms/step - accuracy: 0.8880 - loss: 0.2682 - val_accuracy: 0.8110 - val_loss: 0.4522\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - accuracy: 0.8925 - loss: 0.2553 - val_accuracy: 0.8270 - val_loss: 0.4258\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 175ms/step - accuracy: 0.9000 - loss: 0.2399 - val_accuracy: 0.8345 - val_loss: 0.4371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1960afdfa10>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_set,validation_data=test_set,epochs=25)\n",
    "# validation mtlb jispe test krna hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f544138e-e38d-4ff2-90ce-2df309a4001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image=image.load_img('single_prediction/cat_or_dog.jpg',target_size=(64,64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=cnn.predict(test_image)\n",
    "train_set.class_indices\n",
    "if result [0][0]==1:\n",
    "    prediction='dog'\n",
    "else:\n",
    "    prediction='cat'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71232ab-f774-449c-a84a-a4b2ea398445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
